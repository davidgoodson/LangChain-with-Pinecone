{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e1d420f-c1d4-4375-93dd-895640775b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredEPubLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "import pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\") #langchain searches for this key and loads it automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44e06128-4e47-413e-91ed-c63013544376",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load PDF File\n",
    "loader = PyPDFLoader('./assets/srp-covid-19-6month.pdf')\n",
    "pdf = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afa957d5-ba4f-4f60-bc55-6d8d79af91e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform pdf texst into chunks and create document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eac5a07f-765b-4383-9ecd-deda9b6b05b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vector Embedding and Querying with ChromaDB\n",
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "# Vector database\n",
    "query = \"When was the first COVID case discovered in the united states?\" \n",
    "result = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f05cdc87-1bdf-4bdb-858e-67842a73d28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# epub_loader = UnstructuredEPubLoader(\n",
    "#     file_path=\"./RAG/assets/dokumen.pub_beginning-python-from-novice-to-professional-3rd-edition.epub\", \n",
    "#     mode=\"elements\", \n",
    "#     strategy=\"fast\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "302b3fb5-71f8-4e82-baee-f565147b02d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Vector Embedding and Querying with Pinecone\n",
    "\n",
    "#Setting up pinecone\n",
    "pc = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "    \n",
    "# Check if index exsits before creation\n",
    "index_name = \"rag-project\"\n",
    "if index_name not in [index.name for index in pc.list_indexes()]:\n",
    "    pc.create_index(\n",
    "        name=index_name, \n",
    "        dimension=1536, # Must match ada-002 output\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\", \n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    time.sleep(5) # Wait for index to be ready\n",
    "    \n",
    "# Connect to the index\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aea4c49d-9a79-4c1d-9e2e-9e93c8953861",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Use \"text-embedding-ada-002\"\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Generate embeddings for documents\n",
    "docs = [doc.page_content for doc in documents]\n",
    "embeddings = embed_model.embed_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4a1791",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for PineconeHybridSearchRetriever\nsearch_kwargs\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'k': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 14\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PineconeHybridSearchRetriever\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# vector_store = pinecone.Pinecone.from_existing_index(\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     index_name=index_name,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     embedding=embed_model, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 results\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m retriever \u001b[38;5;241m=\u001b[39m \u001b[43mPineconeHybridSearchRetriever\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Provide the embedding function for the query\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Retrieve top 3 similar documents\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda3/lib/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for PineconeHybridSearchRetriever\nsearch_kwargs\n  Extra inputs are not permitted [type=extra_forbidden, input_value={'k': 3}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/extra_forbidden"
     ]
    }
   ],
   "source": [
    "# Create The Retriever\n",
    "\n",
    "# Wrap Pinecone index in a LangChain retriever\n",
    "from langchain_pinecone import Pinecone\n",
    "from langchain_community.retrievers import PineconeHybridSearchRetriever\n",
    "\n",
    "# vector_store = pinecone.Pinecone.from_existing_index(\n",
    "#     index_name=index_name,\n",
    "#     embedding=embed_model, \n",
    "#     namespace=\"all-users\"\n",
    "# )\n",
    "\n",
    "# retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  # Retrieve top 3 results\n",
    "retriever = PineconeHybridSearchRetriever(\n",
    "    index=index,\n",
    "    embeddings=OpenAIEmbeddings(),  # Provide the embedding function for the query# Retrieve top 3 similar documents\n",
    "    text_key=\"context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b211e216-41cb-49ec-9c44-5dbee310bc2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 202}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for insertion\n",
    "vectors = [(str(i), embeddings[i], {\"text\": docs[i]}) for i in range(len(docs))]\n",
    "\n",
    "# Insert into Pinecone\n",
    "index.upsert(vectors = vectors, namespace = \"all-users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60e67108-bb86-44ab-b70d-d53bb1e645a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define your query\n",
    "query = \"When was the first COVID case discovered in the United States?\"\n",
    "\n",
    "# Convert the query into an embedding using the same model as the documents\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "query_embedding = embeddings.embed_query(query)\n",
    "\n",
    "# Search the Pinecone index\n",
    "results = index.query(\n",
    "    namespace=\"all-users\",\n",
    "    vector=query_embedding,\n",
    "    top_k=3,\n",
    "    include_values=False,\n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "#print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74708917",
   "metadata": {},
   "source": [
    "#### Recievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b66e7-e624-4a04-ac44-78ee3900c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Ollama LLM\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Load Ollama LAMA2 LLM Model\n",
    "llm = OllamaLLM(model='llama2 ')\n",
    "\n",
    "# Design Chat Prompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Answer the following question based only on the provided context.\n",
    "    Think step by step before providing a detailed answer.\n",
    "    I will tip you $1000 if the user finds the answer helpful.\n",
    "    <context>\n",
    "        {context}\n",
    "    </context>\n",
    "    Question: {input}\"\"\")\n",
    "\n",
    "# Chain Introduction\n",
    "\n",
    "# Create Stuff Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    " \n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Retrievers\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "retrieval_chain.invoke({\"input\": query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (data_en)",
   "language": "python",
   "name": "data_en"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
